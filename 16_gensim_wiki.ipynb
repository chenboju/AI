{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenboju/AI/blob/main/16_gensim_wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssk5BSNbelCF"
      },
      "source": [
        "# Word2Vec實作\n",
        "- 字詞所代表的意義非常多元，在不同狀況下，會代表不同意思。要把多元意思用單一向量表示，則必須要進行word embedding的動作，也就是把高維向量降為低維向量的過程\n",
        "- 之前介紹過，利用分散式表示法來表達字詞向量，例如PMI、SVD..統計法..等\n",
        "- 2013年神經網路盛行後，Tomas Mikolov利用神經網路訓練方式，來獲得字詞的表達向量，獲得很棒的成果。一般認為是利用神經網路模擬人類的理解能力，獲得不錯的分布空間所得到的成果。\n",
        "- 本範例以維基百科wiki部分資料作範例\n",
        "- 資料來源：https://dumps.wikimedia.org/zhwiki/20231201/zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2\n",
        "- 利用結巴分詞(jieba)進行斷詞，gensim套件進行word2vec計算\n",
        "- 本範例約需1小時長時間執行\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dumps.wikimedia.org/zhwiki/20240501/zhwiki-20240501-pages-articles-multistream1.xml-p1p187712.bz2"
      ],
      "metadata": {
        "id": "ylVzFulsmgzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8a7e0b-4a5b-438b-a2c4-673586d1ecc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-20 05:54:39--  https://dumps.wikimedia.org/zhwiki/20240501/zhwiki-20240501-pages-articles-multistream1.xml-p1p187712.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 233419414 (223M) [application/octet-stream]\n",
            "Saving to: ‘zhwiki-20240501-pages-articles-multistream1.xml-p1p187712.bz2’\n",
            "\n",
            "zhwiki-20240501-pag 100%[===================>] 222.61M  4.46MB/s    in 54s     \n",
            "\n",
            "2024-05-20 05:55:33 (4.13 MB/s) - ‘zhwiki-20240501-pages-articles-multistream1.xml-p1p187712.bz2’ saved [233419414/233419414]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### opencc是繁簡轉換工具"
      ],
      "metadata": {
        "id": "eMIti7ru3701"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ta_qze2iHJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2629dac6-b3f9-484a-a11c-5b0013732f7d"
      },
      "source": [
        "!pip install opencc-python-reimplemented"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencc-python-reimplemented\n",
            "  Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl (481 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc-python-reimplemented\n",
            "Successfully installed opencc-python-reimplemented-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gensim是訓練word2vec的函式庫"
      ],
      "metadata": {
        "id": "SQCWrmFJ4Mps"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BIoy7K5eqIS"
      },
      "source": [
        "from gensim.corpora import WikiCorpus\n",
        "\n",
        "wiki_corpus = WikiCorpus('zhwiki-20240501-pages-articles-multistream1.xml-p1p187712.bz2', dictionary={})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T7ZZpDr3Sg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131603da-9a71-420c-8961-0f8cb4e3b329"
      },
      "source": [
        "wiki_corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.corpora.wikicorpus.WikiCorpus at 0x7833aa715c00>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqX5g3Idsp-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3ba415-1f75-42fc-f5a7-08b3f05d1e7c"
      },
      "source": [
        "next(iter(wiki_corpus.get_texts()))[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['歐幾里得',\n",
              " '西元前三世紀的古希臘數學家',\n",
              " '而現在被認為是幾何之父',\n",
              " '此畫為拉斐爾的作品',\n",
              " '雅典學院',\n",
              " '数学',\n",
              " '是研究數量',\n",
              " '屬於形式科學的一種',\n",
              " '數學利用抽象化和邏輯推理',\n",
              " '從計數']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 把wiki的資料檔案，轉換成連續文字的txt檔案"
      ],
      "metadata": {
        "id": "AjOTHZGR5nCE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL99YGiSfhGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b0959d-7a40-4c84-d62d-d2479068e8b8"
      },
      "source": [
        "text_num = 0  # 初始化文本計數器，用於跟踪處理的文本數量\n",
        "\n",
        "# 打開名為 'wiki_text.txt' 的文本文件，使用 utf-8 編碼\n",
        "# 並使用迴圈將每個文本寫入文件中\n",
        "with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
        "    for text in wiki_corpus.get_texts():\n",
        "        # 將每個文本的詞彙列表以空格分隔，並寫入文件中\n",
        "        f.write(' '.join(text) + '\\n')\n",
        "\n",
        "        # 每處理完 10000 篇文章就打印出進度信息\n",
        "        text_num += 1\n",
        "        if text_num % 10000 == 0:\n",
        "            print('{} articles processed.'.format(text_num))\n",
        "\n",
        "    # 最後再次打印處理的總文章數\n",
        "    print('{} articles processed.'.format(text_num))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 articles processed.\n",
            "20000 articles processed.\n",
            "30000 articles processed.\n",
            "32786 articles processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenCC: 用於將文本從簡體轉換為繁體，使用了 s2t 的模式。<br>\n",
        "jieba: 用於對文本進行分詞。<br>\n",
        "read(): 讀取文件中的文本。<br>\n",
        "lcut(): 對文本進行分詞。<br>\n",
        "過濾空字符: 通過列表推導式過濾掉分詞結果中的空字符。<br>\n",
        "join(): 將分詞結果連接成一個字符串，並用空格分隔。<br>\n",
        "write(): 將處理後的文本寫入到 seg.txt 文件中。<br>"
      ],
      "metadata": {
        "id": "WpFSf1J_p2iy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ01OEi6szXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798d7280-6083-494b-efd1-14952f7260eb"
      },
      "source": [
        "import jieba  # 引入 jieba 库，用于中文分词\n",
        "from opencc import OpenCC  # 引入 OpenCC 库，用于簡繁轉換\n",
        "\n",
        "# 初始化 OpenCC，設置從簡體到繁體的轉換\n",
        "cc = OpenCC('s2t')\n",
        "\n",
        "# 讀取文件中的訓練數據，並將其進行簡體轉換\n",
        "train_data = open('wiki_text.txt', 'r', encoding='utf-8').read()\n",
        "train_data = cc.convert(train_data)\n",
        "\n",
        "# 使用 jieba 對文本進行分詞\n",
        "train_data = jieba.lcut(train_data)\n",
        "\n",
        "# 過濾空字符\n",
        "train_data = [word for word in train_data if word != '']\n",
        "\n",
        "# 將分詞結果連接成字符串，並用空格分隔\n",
        "train_data = ' '.join(train_data)\n",
        "\n",
        "# 將處理後的文本寫入到文件 'seg.txt' 中\n",
        "open('seg.txt', 'w', encoding='utf-8').write(train_data)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136623985"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXxRBbays3k0"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# 設置參數\n",
        "seed = 666  # 隨機種子\n",
        "sg = 0  # 使用 CBOW 模型\n",
        "window_size = 10  # 上下文窗口大小\n",
        "# vector_size = 100  # 詞向量的維度\n",
        "min_count = 1  # 最小詞頻閾值，小於這個值的詞將會被忽略\n",
        "workers = 8  # 訓練時使用的線程數\n",
        "# epochs = 5  # 訓練的迭代次數\n",
        "batch_words = 10000  # 每次訓練時的單詞數\n",
        "\n",
        "# 從 'seg.txt' 文件中讀取訓練數據\n",
        "train_data = word2vec.LineSentence('seg.txt')\n",
        "\n",
        "# 使用 Word2Vec 模型進行訓練\n",
        "model = word2vec.Word2Vec(\n",
        "    train_data,\n",
        "    min_count=min_count,\n",
        "    # size=vector_size,\n",
        "    workers=workers,\n",
        "    # iter=epochs,\n",
        "    window=window_size,\n",
        "    sg=sg,\n",
        "    seed=seed,\n",
        "    batch_words=batch_words\n",
        ")\n",
        "\n",
        "# 將訓練好的模型保存到文件中\n",
        "model.save('word2vec.model')\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PovTacQs-R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafae1c5-7090-4dc2-c176-06665918fa17"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# 載入訓練好的詞向量模型\n",
        "model = word2vec.Word2Vec.load('word2vec.model')\n",
        "\n",
        "# 要查找相似詞彙的詞彙\n",
        "string = '電腦'\n",
        "\n",
        "# 輸出要查找相似詞彙的詞彙\n",
        "print(string)\n",
        "\n",
        "# 使用模型的 most_similar 方法查找與指定詞彙最相似的詞彙\n",
        "for item in model.wv.most_similar(string):\n",
        "    print(item)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "電腦\n",
            "('計算機', 0.7695195078849792)\n",
            "('pda', 0.7647251486778259)\n",
            "('軟體', 0.7627525329589844)\n",
            "('ibm', 0.7318840026855469)\n",
            "('程式', 0.7282395362854004)\n",
            "('硬體', 0.7276607751846313)\n",
            "('家用', 0.7192538976669312)\n",
            "('遊戲機', 0.7177554965019226)\n",
            "('模擬器', 0.7122527956962585)\n",
            "('pc', 0.7100153565406799)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E235am9EVIuG"
      },
      "source": [],
      "execution_count": 9,
      "outputs": []
    }
  ]
}